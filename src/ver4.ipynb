{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-19T04:52:33.514297Z",
     "start_time": "2025-12-19T04:52:22.356104Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from scipy.stats import rankdata\n",
    "from scipy import sparse\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T04:52:34.773351Z",
     "start_time": "2025-12-19T04:52:33.521573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"../data/raw/fraud_dataset_v1.csv\")\n",
    "\n",
    "labels_orig = df[\"label\"].values        # ONLY for evaluation\n",
    "df_feat = df.drop(columns=[\"label\"])\n",
    "\n",
    "num_cols = df_feat.select_dtypes(include=np.number).columns\n",
    "cat_cols = df_feat.select_dtypes(include=object).columns\n",
    "\n",
    "X_num = StandardScaler().fit_transform(df_feat[num_cols])\n",
    "\n",
    "if len(cat_cols) > 0:\n",
    "    enc = OneHotEncoder(sparse_output=True, handle_unknown=\"ignore\")\n",
    "    X_cat = enc.fit_transform(df_feat[cat_cols])\n",
    "    X = sparse.hstack([sparse.csr_matrix(X_num), X_cat]).tocsr()\n",
    "else:\n",
    "    X = sparse.csr_matrix(X_num)\n",
    "\n",
    "X_dense = X.astype(np.float32).toarray()\n",
    "N, input_dim = X_dense.shape\n"
   ],
   "id": "3bc60488e5ca58fb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T04:52:35.200607Z",
     "start_time": "2025-12-19T04:52:35.150127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = TensorDataset(torch.from_numpy(X_dense))\n",
    "train_size = int(0.8 * N)\n",
    "val_size = N - train_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=1024, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=1024)\n"
   ],
   "id": "3dacdd499909bca4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T04:52:35.215051Z",
     "start_time": "2025-12-19T04:52:35.209458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=8):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = torch.randn_like(x) * 0.05\n",
    "        x_noisy = x + noise\n",
    "        z = self.encoder(x_noisy)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z\n"
   ],
   "id": "3d7c14849bcf9447",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T05:02:14.680457Z",
     "start_time": "2025-12-19T04:52:35.229553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ae = DenoisingAutoencoder(input_dim, latent_dim=8)\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "best_val = np.inf\n",
    "patience, counter = 5, 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(50):\n",
    "    ae.train()\n",
    "    train_loss = 0\n",
    "    for (x,) in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, _ = ae(x)\n",
    "        loss = criterion(x_hat, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "\n",
    "    ae.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (x,) in val_loader:\n",
    "            x_hat, _ = ae(x)\n",
    "            val_loss += criterion(x_hat, x).item() * x.size(0)\n",
    "\n",
    "    train_loss /= train_size\n",
    "    val_loss   /= val_size\n",
    "    print(f\"Epoch {epoch+1} | Train {train_loss:.6f} | Val {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        best_state = ae.state_dict()\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "ae.load_state_dict(best_state)\n",
    "ae.eval()\n"
   ],
   "id": "1978db453b2baa67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train 0.004507 | Val 0.002604\n",
      "Epoch 2 | Train 0.002286 | Val 0.002183\n",
      "Epoch 3 | Train 0.001593 | Val 0.001363\n",
      "Epoch 4 | Train 0.001303 | Val 0.001274\n",
      "Epoch 5 | Train 0.001258 | Val 0.001207\n",
      "Epoch 6 | Train 0.001271 | Val 0.001183\n",
      "Epoch 7 | Train 0.001160 | Val 0.001107\n",
      "Epoch 8 | Train 0.001101 | Val 0.001065\n",
      "Epoch 9 | Train 0.001054 | Val 0.000984\n",
      "Epoch 10 | Train 0.000966 | Val 0.001009\n",
      "Epoch 11 | Train 0.000930 | Val 0.000872\n",
      "Epoch 12 | Train 0.000834 | Val 0.000790\n",
      "Epoch 13 | Train 0.000772 | Val 0.000731\n",
      "Epoch 14 | Train 0.000762 | Val 0.001742\n",
      "Epoch 15 | Train 0.000736 | Val 0.000618\n",
      "Epoch 16 | Train 0.000599 | Val 0.000577\n",
      "Epoch 17 | Train 0.000559 | Val 0.000539\n",
      "Epoch 18 | Train 0.000519 | Val 0.000495\n",
      "Epoch 19 | Train 0.000476 | Val 0.000461\n",
      "Epoch 20 | Train 0.000473 | Val 0.000479\n",
      "Epoch 21 | Train 0.000414 | Val 0.000394\n",
      "Epoch 22 | Train 0.000400 | Val 0.000374\n",
      "Epoch 23 | Train 0.000351 | Val 0.000335\n",
      "Epoch 24 | Train 0.000329 | Val 0.000366\n",
      "Epoch 25 | Train 0.000323 | Val 0.000351\n",
      "Epoch 26 | Train 0.000306 | Val 0.000275\n",
      "Epoch 27 | Train 0.000265 | Val 0.000290\n",
      "Epoch 28 | Train 0.000259 | Val 0.000241\n",
      "Epoch 29 | Train 0.000240 | Val 0.000234\n",
      "Epoch 30 | Train 0.000267 | Val 0.000231\n",
      "Epoch 31 | Train 0.000219 | Val 0.000212\n",
      "Epoch 32 | Train 0.000204 | Val 0.000234\n",
      "Epoch 33 | Train 0.000195 | Val 0.000185\n",
      "Epoch 34 | Train 0.000182 | Val 0.000206\n",
      "Epoch 35 | Train 0.000221 | Val 0.000180\n",
      "Epoch 36 | Train 0.000167 | Val 0.000162\n",
      "Epoch 37 | Train 0.000161 | Val 0.000165\n",
      "Epoch 38 | Train 0.000163 | Val 0.000165\n",
      "Epoch 39 | Train 0.000146 | Val 0.000147\n",
      "Epoch 40 | Train 0.000161 | Val 0.000147\n",
      "Epoch 41 | Train 0.000144 | Val 0.000163\n",
      "Epoch 42 | Train 0.000138 | Val 0.000132\n",
      "Epoch 43 | Train 0.000135 | Val 0.000130\n",
      "Epoch 44 | Train 0.000130 | Val 0.000135\n",
      "Epoch 45 | Train 0.000170 | Val 0.000129\n",
      "Epoch 46 | Train 0.000123 | Val 0.000117\n",
      "Epoch 47 | Train 0.000126 | Val 0.000122\n",
      "Epoch 48 | Train 0.000114 | Val 0.000114\n",
      "Epoch 49 | Train 0.000116 | Val 0.000114\n",
      "Epoch 50 | Train 0.000111 | Val 0.000126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenoisingAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=618, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=8, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=618, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T05:02:19.661983Z",
     "start_time": "2025-12-19T05:02:14.748217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rec_error, latent = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, N, 1024):\n",
    "        batch = torch.from_numpy(X_dense[i:i+1024])\n",
    "        x_hat, z = ae(batch)\n",
    "        rec_error.append(((batch - x_hat)**2).mean(dim=1).numpy())\n",
    "        latent.append(z.numpy())\n",
    "\n",
    "rec_error = np.concatenate(rec_error)\n",
    "latent = np.concatenate(latent)\n"
   ],
   "id": "646292158286445a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T05:02:36.290152Z",
     "start_time": "2025-12-19T05:02:19.727045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if_model = IsolationForest(\n",
    "    n_estimators=300,\n",
    "    contamination=0.01,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "if_model.fit(latent)\n",
    "if_score = -if_model.score_samples(latent)\n",
    "\n",
    "rec_rank = rankdata(rec_error) / N\n",
    "if_rank  = rankdata(if_score) / N\n",
    "\n",
    "ensemble_global = 0.6 * rec_rank + 0.4 * if_rank\n"
   ],
   "id": "b5f72efc7864afbb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T05:03:07.357826Z",
     "start_time": "2025-12-19T05:02:36.332913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_lof = np.column_stack([latent, rec_error])\n",
    "\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=35,\n",
    "    metric=\"euclidean\"\n",
    ")\n",
    "\n",
    "lof_score = -lof.fit_predict(X_lof)\n",
    "lof_rank = rankdata(lof_score) / N\n"
   ],
   "id": "2c2ffcba0d384f4e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T05:03:07.406937Z",
     "start_time": "2025-12-19T05:03:07.403712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TH_GLOBAL = 0.9985\n",
    "pred_global = ensemble_global >= TH_GLOBAL\n"
   ],
   "id": "a50e133e216a1081",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T05:03:07.415785Z",
     "start_time": "2025-12-19T05:03:07.412629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TH_LOCAL = 0.999\n",
    "pred_local_candidate = lof_rank >= TH_LOCAL\n"
   ],
   "id": "9ccfde91024f027",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T05:03:07.424949Z",
     "start_time": "2025-12-19T05:03:07.421536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "K_LOCAL = int(0.001 * N)\n",
    "\n",
    "local_idx = np.where(pred_local_candidate)[0]\n",
    "if len(local_idx) > K_LOCAL:\n",
    "    local_idx = local_idx[np.argsort(lof_rank[local_idx])[-K_LOCAL:]]\n",
    "\n",
    "pred_local = np.zeros(N, dtype=bool)\n",
    "pred_local[local_idx] = True\n"
   ],
   "id": "cb5e7fea36c56cb4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T05:03:07.433955Z",
     "start_time": "2025-12-19T05:03:07.430084Z"
    }
   },
   "cell_type": "code",
   "source": "preds = (pred_global | pred_local).astype(int)\n",
   "id": "2a7f4cccbfd0364a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T05:03:07.728451Z",
     "start_time": "2025-12-19T05:03:07.439587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_map = {\"regular\": 0, \"local\": 1, \"global\": 1}\n",
    "labels_numeric = np.array([label_map[l] for l in labels_orig])\n",
    "\n",
    "print(\"Anomalies predicted:\", preds.sum())\n",
    "print(\"Precision:\", precision_score(labels_numeric, preds))\n",
    "print(\"Recall:\", recall_score(labels_numeric, preds))\n",
    "print(\"F1:\", f1_score(labels_numeric, preds))\n",
    "print(\"ROC-AUC:\", roc_auc_score(labels_numeric, ensemble_global))\n",
    "\n",
    "print(\"\\nDetected anomalies by type:\")\n",
    "print(pd.Series(labels_orig[preds == 1]).value_counts())\n"
   ],
   "id": "a93bf757a27cd06b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies predicted: 269\n",
      "Precision: 0.26022304832713755\n",
      "Recall: 0.7\n",
      "F1: 0.3794037940379404\n",
      "ROC-AUC: 0.8867683131641613\n",
      "\n",
      "Detected anomalies by type:\n",
      "regular    199\n",
      "global      70\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
